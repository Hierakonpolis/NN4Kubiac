{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003ab30f",
   "metadata": {},
   "source": [
    "# Cross validation in scikit-learn\n",
    "### In this tutorial\n",
    "- What not to do\n",
    "- K-Fold cross validation\n",
    "- Stratified K-Fold cross validation\n",
    "- Nested stratified K-Fold cross validation\n",
    "\n",
    "## What not to do\n",
    "We will train a simple classifier on the iris dataset, one of the standard datasets included in scikit-learn. We will start by training and testing on the same data. Let't import the data, scikit-learn and pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da97601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use an example dataset incluided in scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load scikit's random forest classifier library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b3b39",
   "metadata": {},
   "source": [
    "We will now load the dataset and have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00068a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Prepare the data\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "features = df.columns[:4]\n",
    "\n",
    "print(len(df),'samples')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a399659",
   "metadata": {},
   "source": [
    "We have four features for every flower, and we are trying to recognize which species of iris it belongs to. Let's try a random forest as a classification algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c926de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classifier:\n",
    "trees = 2\n",
    "jobs = 20\n",
    "forest = RandomForestClassifier(trees, n_jobs=jobs)\n",
    "\n",
    "# Something we should never do: train and test on the same data\n",
    "\n",
    "forest.fit(df[features],df['species'])\n",
    "results = forest.predict(df[features])\n",
    "train_accuracy = accuracy_score(results,df['species'])\n",
    "print(results[48:52])\n",
    "print('Train accuracy score:',train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8053a",
   "metadata": {},
   "source": [
    "Looks good! But how would it perform on new data? By using all of our data for training, we cannot answer this question. A common strategy would be to split our data in a training set and a test set. However, when our dataset is small, we often want to somehow use all of our data both for training and testing. To do this, we can use...\n",
    "\n",
    "## K-Fold cross validation\n",
    "\n",
    "The idea is simple: we split the data in K subsets, and select each one in turn. We test on that subset, and train using the remaining data. Our final evaluation of the classifier is the average across all subsets, or **folds**.\n",
    "\n",
    "For K = 5 we have:\n",
    "\n",
    "<img src=\"folds.png\">\n",
    "\n",
    "Let's take a look at how we can do this in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a49c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "splitter = KFold(5,shuffle=False) # Let's see how this works\n",
    "\n",
    "train, test = next(splitter.split(df))\n",
    "print('Train',train,'\\n\\nTest',test)\n",
    "df.iloc[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de513cab",
   "metadata": {},
   "source": [
    "## Stratified K-Fold\n",
    "Stratified K-Folds work in exactly the same way, however the samples in each fold are chosen to preserve the percentage of samples for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3917270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified k-fold\n",
    "splitter = StratifiedKFold(5,shuffle=False) # Let's see the effects of shuffle\n",
    "\n",
    "train, test = next(splitter.split(df,y = df['species']))\n",
    "print('Train',train,'\\n\\nTest',test)\n",
    "df.iloc[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53429cb",
   "metadata": {},
   "source": [
    "As an exercise, let's implement 5-fold stratified cross validation using scikit-learn. We need to:\n",
    "\n",
    "- Loop over all folds\n",
    "- Train the random forest using the training indices\n",
    "- Evaluate on the testing data\n",
    "- Save the prediction and the ground truth, or alternatively record the accuracy\n",
    "- Return the average accuracy across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Exercise: implementing stratified K-fold CV\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for train, test in splitter.split(df,y = df['species']):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16dd8a6",
   "metadata": {},
   "source": [
    "## Nested stratified K-Fold CV\n",
    "\n",
    "Sometimes we need to optimize for the hyperparameters of our classifiers, for example using a grid search. Scikit-learn makes it easy to combine a grid search with cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4188544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'n_estimators': [1,10,100,1000,10000], 'max_depth': (1,2,None)}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=RandomForestClassifier(n_jobs=jobs),\n",
    "            param_grid=parameters,\n",
    "            cv = StratifiedKFold(10,shuffle=True),\n",
    "            verbose = 3\n",
    "            )\n",
    "\n",
    "gs.fit(df[features],df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score',gs.best_score_,'\\nParameters:', gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9934373",
   "metadata": {},
   "source": [
    "This comes with a problem: the data we just used was also involved in the optimization process, as we kept testing on the same data through the whole grid. We need two hold-out datasets. A dev set, to test the algorithm as we explore the grid, and a final test set, to evaluate the result we selected as the optimum on new data the algorithm has not seen before.\n",
    "\n",
    "We do so by nesting a stratified K-Fold CV inside another stratified K-Fold CV. We first split our data in a training and test set, and further split the training set into a training and a dev set. In practice, we just need to build a loop as we did before, but for each iteration we will run a grid search with its own inner CV. Let's do this now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Exercise: nested stratified K-fold CV\n",
    "parameters = {'n_estimators': [10,100,1000], 'max_depth': (1,2,None)}\n",
    "outer = StratifiedKFold(10,shuffle=True)\n",
    "inner = StratifiedKFold(10,shuffle=True)\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "import tqdm\n",
    "for train, test in ...:\n",
    "    pass\n",
    "\n",
    "test_accuracy = accuracy_score(predictions,targets)\n",
    "print('Test accuracy',test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
